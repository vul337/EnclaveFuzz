
#include "Malloc.h"
#include "Poison.h"
#include <algorithm>
#include <string.h>
#include <unordered_set>

#define DEFINE_BACK_END(sym) decltype(sym) *BACK_END(sym)
DEFINE_BACK_END(malloc) = __libc_malloc;
DEFINE_BACK_END(free) = __libc_free;
DEFINE_BACK_END(calloc) = __libc_calloc;
DEFINE_BACK_END(realloc) = __libc_realloc;
DEFINE_BACK_END(malloc_usable_size) = nullptr;
#undef DEFINE_BACK_END

bool alreadyUpdateBackEndHeapAllocator = false;
void updateBackEndHeapAllocator() {
  // since we also update it in SGXSan ctor, so this statement only will be
  // triggered before SGXSan's ctor, and only one main thread exists, thus,
  // needn't consider multi-thread situation
  if (alreadyUpdateBackEndHeapAllocator)
    return;
#define GET_BACK_END(sym)                                                      \
  sgxsan_assert(BACK_END(sym) = (decltype(sym) *)dlsym(RTLD_NEXT, #sym))
  GET_BACK_END(malloc);
  GET_BACK_END(free);
  GET_BACK_END(calloc);
  GET_BACK_END(realloc);
  GET_BACK_END(malloc_usable_size);
#undef GET_BACK_END
  alreadyUpdateBackEndHeapAllocator = true;
}

static QuarantineCache QCache;
QuarantineCache *gQCache = &QCache;

static pthread_mutex_t heap_usage_mutex = PTHREAD_MUTEX_INITIALIZER;
size_t global_heap_usage = 0;
const size_t kHeapObjectChunkMagic = 0xDEADBEEF;

struct chunk {
  size_t magic; // ensure queried user_beg is correct
  uptr alloc_beg;
  size_t alloc_size;
  size_t user_size;
};

void update_heap_usage(void *ptr, bool true_add_false_minus) {
#if (USED_LOG_LEVEL >= 3 /* LOG_LEVEL_DEBUG */)
  static uint64_t heapLogIndex = 0;
  if (ptr) {
    pthread_mutex_lock(&heap_usage_mutex);
    heapLogIndex++;
    size_t allocated_size = BACKEND_MALLOC_USABLE_SIZE(ptr);
    if (true_add_false_minus) {
      log_trace("(%ld)[HEAP SIZE] 0x%lx=0x%lx+0x%lx\n", heapLogIndex,
                global_heap_usage + allocated_size, global_heap_usage,
                allocated_size);
      global_heap_usage += allocated_size;
    } else {
      log_trace("(%ld)[HEAP SIZE] 0x%lx=0x%lx-0x%lx\n", heapLogIndex,
                global_heap_usage - allocated_size, global_heap_usage,
                allocated_size);
      global_heap_usage -= allocated_size;
    }
    pthread_mutex_unlock(&heap_usage_mutex);
  }
#else
  (void)ptr;
  (void)true_add_false_minus;
  (void)heap_usage_mutex;
#endif
}

void *MALLOC(size_t size) {
  updateBackEndHeapAllocator();
  if (size == 0) {
    sgxsan_warning(true, "Malloc 0 size\n");
    // return nullptr;
  }

  if (not asan_inited) {
    auto p = BACKEND_MALLOC(size);
    update_heap_usage(p);
    return p;
  }

  uptr alignment = SHADOW_GRANULARITY;

  uptr rz_size =
      std::max(ComputeRZSize(size), RoundUpTo(sizeof(chunk), alignment));
  uptr rounded_size = RoundUpTo(size, alignment);
  uptr needed_size = rounded_size + 2 * rz_size;

  void *allocated = BACKEND_MALLOC(needed_size);
  if (allocated == nullptr) {
    return nullptr;
  }
  update_heap_usage(allocated);

  size_t allocated_size = BACKEND_MALLOC_USABLE_SIZE(allocated);

  uptr alloc_beg = (uptr)allocated;
  // If real malloc doesn't return an aligned memory, it's troublesome.
  sgxsan_assert(IsAligned(alloc_beg, alignment));
  uptr alloc_end = alloc_beg + allocated_size;

  uptr user_beg = alloc_beg + rz_size;
  if (!IsAligned(user_beg, alignment))
    user_beg = RoundUpTo(user_beg, alignment);
  uptr user_end = user_beg + size;
  sgxsan_assert(user_end <= alloc_end);

  // place the chunk in left redzone
  uptr chunk_beg = user_beg - sizeof(chunk);
  chunk *m = (chunk *)chunk_beg;

  m->magic = kHeapObjectChunkMagic;
  m->alloc_beg = alloc_beg;
  m->alloc_size = allocated_size;
  m->user_size = size;
  log_trace("\n");
  log_trace("[Malloc] [0x%lx..0x%lx ~ 0x%lx..0x%lx)\n", alloc_beg, user_beg,
            user_end, alloc_end);

  PoisonShadow(alloc_beg, user_beg - alloc_beg, kAsanHeapLeftRedzoneMagic);
  sgxsan_assert(IsAligned(user_beg, alignment));
  PoisonShadow(user_beg, size, kAsanNotPoisonedMagic);
  uptr right_redzone_beg = RoundUpTo(user_end, alignment);
  PoisonShadow(right_redzone_beg, alloc_end - right_redzone_beg,
               kAsanHeapRightRedzoneMagic);

  return (void *)user_beg;
}

void FREE(void *ptr) {
  uptr user_beg, alignment;
  chunk *m;
  QuarantineElement qe;
  updateBackEndHeapAllocator();
  if (not asan_inited) {
    goto fallback;
  }
  if (ptr == nullptr)
    return;

  user_beg = (uptr)ptr;
  alignment = SHADOW_GRANULARITY;

  m = (chunk *)(user_beg - sizeof(chunk));
  if (m->magic != kHeapObjectChunkMagic) {
    // It may be malloced before SGXSan's init, directly free it, and leave
    // normal free to check pointer
    goto fallback;
  }

  if (*(uint8_t *)MEM_TO_SHADOW(user_beg) == kAsanHeapFreeMagic) {
    GET_CALLER_PC_BP_SP;
    ReportGenericError(pc, bp, sp, user_beg, 0, 1, true, "Double Free");
  }
  sgxsan_assert(IsAligned(user_beg, alignment));

  log_trace("\n");
  log_trace("[Recycle] [0x%lx..0x%lx ~ 0x%lx..0x%lx)\n", m->alloc_beg, user_beg,
            user_beg + m->user_size, m->alloc_beg + m->alloc_size);
  PoisonShadow(user_beg, RoundUpTo(m->user_size, alignment),
               kAsanHeapFreeMagic);

  qe.alloc_beg = m->alloc_beg;
  qe.alloc_size = m->alloc_size;
  qe.user_beg = user_beg;
  qe.user_size = m->user_size;

  gQCache->put(qe);
  goto exit;

fallback:
  update_heap_usage(ptr, false);
  BACKEND_FREE(ptr);
exit:
  return;
}

void *CALLOC(size_t n_elements, size_t elem_size) {
  // Since dlsym use calloc, so avoid call updateBackEndHeapAllocator and dlsym
  // in it, and we directly use __libc_calloc as backend calloc
  if (not asan_inited) {
    return BACKEND_CALLOC(n_elements, elem_size);
  }

  size_t req = n_elements * elem_size;
  if (req == 0) {
    sgxsan_warning(true, "Calloc 0 size\n");
    return nullptr;
  }
  sgxsan_assert(req / n_elements == elem_size);
  void *mem = MALLOC(req);
  if (mem != nullptr) {
    memset(mem, 0, req);
  }
  return mem;
}

void *REALLOC(void *oldmem, size_t bytes) {
  updateBackEndHeapAllocator();
  if (not asan_inited) {
    return BACKEND_REALLOC(oldmem, bytes);
  }

  if (oldmem == nullptr) {
    return MALLOC(bytes);
  }
  chunk *m = (chunk *)((uptr)oldmem - sizeof(chunk));
  sgxsan_assert(m->magic == kHeapObjectChunkMagic);
  if (bytes == 0) {
    FREE(oldmem);
    return nullptr;
  }

  void *mem = MALLOC(bytes);

  if (mem != nullptr) {
    memcpy(mem, oldmem, std::min(m->user_size, bytes));
    FREE(oldmem);
  }

  return mem;
}

size_t MALLOC_USABLE_SIZE(void *mem) {
  updateBackEndHeapAllocator();
  chunk *m = (chunk *)((uptr)mem - sizeof(chunk));
  sgxsan_assert(m->magic == kHeapObjectChunkMagic);
  return m->user_size;
}
